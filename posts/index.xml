<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Aileen&#39;s Blog</title>
    <link>https://aileenxie.github.io/posts/</link>
    <description>Recent content in Posts on Aileen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch-cn</language>
    <lastBuildDate>Wed, 10 Oct 2018 19:17:39 +0800</lastBuildDate>
    
	<atom:link href="https://aileenxie.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>L0,L1,L2以及核范数规则化</title>
      <link>https://aileenxie.github.io/2018/03norm-introduction/</link>
      <pubDate>Wed, 10 Oct 2018 19:17:39 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/03norm-introduction/</guid>
      <description>看到一篇讲解很详细的文章，边看边做笔记整理要点。
前言 监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。
最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。
 过拟合：参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力。 泛化能力强：测试误差也很小 范数规则化有两个作用：
1）保证模型尽可能的简单，避免过拟合。
2）约束模型特性，加入一些先验知识，例如稀疏、低秩等。
  目标函数 一般来说，监督学习可以看做最小化下面的目标函数：  第一项L(yi,f(xi;w)): 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。
 如果是Square loss,那就是最小二乘了； 如果是Hinge Loss，那就是著名的SVM了； 如果是exp-Loss，那就是牛逼的 Boosting了； 如果是log-Loss，那就是Logistic Regression了； 不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。
  第二项λΩ(w): 也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。
 本文讨论的即是“规则项Ω(w)”； 一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数； 论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等；    L0范数与L1范数 L0范数 L0范数（||W||0）是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。
L1范数 L1范数（||W||1）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。L1范数会使权值稀疏，它是L0范数的最优凸近似。
 Tips: 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微。  为什么不用L0，而用L1？  是因为L0范数很难优化求解（NP难问题）； 是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。 总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。  为什么要稀疏？  特征选择(Feature Selection)：去掉没有信息的特征，及将对应权重置0，防止无用特征对测试新样本的干扰； 可解释性(Interpretability)：如最初有1000个特征，回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b，通过学习，如果最后学习到只有5个非零的wi，那么就可以说影响患病率的主要就是这5个特征，医生就好分析多了。  L2范数 L2范数（||W||2）是指向量各元素的平方和然后求平方根。它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”（weight decay）它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。与L1范数不同，它不会让它等于0，而是接近于0。</description>
    </item>
    
    <item>
      <title>Hugo快速搭建博客</title>
      <link>https://aileenxie.github.io/2018/hugo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Fri, 05 Oct 2018 21:52:20 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/hugo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>简单记录一下mac下用Hugo搭建博客的过程，以便日后查阅。感谢renyijiu的安利和帮助。
Hugo  一个用Go语言写的静态网站生成器 能把markdown转变成静态网页 内置web服务期，便于本地草稿调试  安装 Hugo  直接用Homebrew安装
brew install hugo  完成之后查看版本，我目前装的是0.49
hugo version  创建一个新站点 到你的目录下找个喜欢的地方执行语句
hugo new site myblog   会在当前目录生成一个叫“myblog”的目录，里面包含若干文件夹和一个config.toml文件:
▸ archetypes/
▸ content/ -&amp;gt;你写的markdown文章
▸ layouts/ -&amp;gt;网站的模板文件
▸ static/ -&amp;gt;放的是一些图片、css、js等资源
▸ data/
▸ themes/ -&amp;gt;放的是你之后添加的主题
config.toml -&amp;gt;网站的配置文件
添加主题  进入目录页
cd myblog  初始化为git项目，方便之后放入github或者任何仓库进行版本管理
git init  添加主题, 以LeaveIt为例
git submodule add https://github.com/liuzc/LeaveIt.git themes/LeaveIt  之后会在themes/目录下看到你添加的主题。
 修改配置文件(直接打开config.toml文件修改也是一样的)
echo &#39;theme = &amp;quot;LeaveIt&amp;quot;&#39; &amp;gt;&amp;gt; config.</description>
    </item>
    
    <item>
      <title>New Start</title>
      <link>https://aileenxie.github.io/2018/first-post/</link>
      <pubDate>Fri, 05 Oct 2018 13:01:24 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/first-post/</guid>
      <description>重新回到学校一个月，第一个国庆避开人山人海，在上海安安心心休息了几天。
之前工作的时候一直想要搭一个博客，把每天零零碎碎记录的笔记整理出来放在一起，但一直没能着手开始行动（大概是又忙又懒。。）直到笔记随手记了一大堆，越来越不想整理。
回到学校自由时间多了很多，又在学习新东西，想着这是个好的时机开始写博客，终于在这个国庆开始了这项活动。
关于笔记  最开始一年我用的ubuntu系统，被安利了cherrytree，试用了一下，马上被这种可以“遍地开花”随手快速记录的树状结构笔记软件所吸引。 那年我刚工作，又是从通信跨到互联网行业，很多东西对我来说都是新知识：数据库(Mysql/Postgres)、编程语言（JAVA/Groov/Python)、测试框架（Spock/Selenium/Appium/rf/jmeter)、版本管理系统（Git）、持续集成（Gradle/Jenkins）包括Linux系统本身&amp;hellip; 用cherrytree记了相当多的笔记，这一年也是我收获颇丰的一年。 第二年换了macOS系统，cherrytree用不了（用模拟器打开页面很丑），先后换了有道云笔记，为知笔记，最后定格在了bootsnote（页面简洁好看，支持markdown和snippet）。又陆续做了关于ruby系测试框架（Cucumber/Capybara）、mac系统等的笔记。直到开学，我作为测试开发的工作告一段落。笔记也就此暂停。  关于博客  终于展开新的研究生生活，投身于机器学习、深度学习和计算机视觉方向研究。有种可以大展拳脚的感觉略兴奋。从开学前到现在，看过了CS229吴恩达的机器学习基础课程，CS231n斯坦福卷积神经网络课程，学了pytorch框架，开始看导师给的paper。开始看的很细致做的手写的笔记，现在想来还是要有个地方集中放置一下，就从现在开始重新规划一下之后的笔记。预计会放一些paper的阅读笔记，学习的知识点，或许是任何想记下来的东西吧hhh&amp;hellip;  七天假期马上要过去啦，搭好了博客算是一个很好的开始，希望能我好好充实这个博客，记录整个研究生阶段以及未来和以后更远更远的学习旅程，(๑•̀ㅂ•́)و✧加油！！</description>
    </item>
    
  </channel>
</rss>