<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Aileen&#39;s Blog</title>
    <link>https://aileenxie.github.io/posts/</link>
    <description>Recent content in Posts on Aileen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch-cn</language>
    <lastBuildDate>Wed, 29 Jan 2020 20:59:19 +0800</lastBuildDate>
    
	<atom:link href="https://aileenxie.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>SCI分区理解</title>
      <link>https://aileenxie.github.io/2020/14sci/</link>
      <pubDate>Wed, 29 Jan 2020 20:59:19 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2020/14sci/</guid>
      <description>之前一直没有仔细了解过SCI分区，概念一直很模糊，一区二区啥的没什么明确定义。
这次查过资料明确一下这个定义。
首先两个个概念：  SCI：全称是Science Citation Index（科学引文索引），科学引文索引是由美国科学资讯研究所于1960年上线投入使用的一部期刊文献检索工具。 目前由科睿唯安公司运营。被SCI收录的论文在一定程度上也代表相对高质量的论文。 JCR：全称是Journal Citation Reports（期刊引证报告），汤森路透社每年对SCI收录的期刊进行引用和被引用数据进行统计计算，并以影响因子（IF，Impact Factor，反映期刊文献被引用率的高低=期刊总引用次数/总文章数，用来说明期刊的影响力）等指数加以报道形成的报告。  简单来说：SCI是一个索引工具，把相对高质量论文收录在一起形成索引；而JCR是根据SCI收录的论文进行指标统计形成的报告，用来评价期刊的质量。
其次为什么要分区 为了公平！ 因为不同学科领域影响因子存在差别，有的热门学科容易出现高影响因子期刊，若不考虑学科之分，混在一起比较就不公平了，所以需要分区。某领域1区的期刊，就是该领域的顶级刊物，能够直观反映其领域内的水准。
目前分区种类 主要是以下两种：
 汤森路透分区 （JCR分区）： JCR将收录期刊分为176个不同学科类别，每个学科类别根据 当年影响因子IF从高到低排序，等分成4等分，排名在前25%是1区，记作Q1；25-50%是2区，记作Q2；50-75%是3区，记作Q3；75%以后的属于4区，Q4。JCR分区可以从web of science进行查询，看期刊具体分区情况。
 中科院期刊分区 (国内用的比较多)：是基于科学计量学方法的期刊评价数据，是中国科学院文献情报中心科学计量中心在汤森路透JCR分区的基础上，选择 学术影响力（3年平均IF） 作为划分方式，把每个学科的所有期刊按照学术影响力由高到底降序排列，依次划分为4个区，使得每个分区期刊影响力总和相同。
  简单来说：
   JCR分区 中科院     Q1/Q2/Q3/Q4每个区期刊数量都一样 1区数量最少，2、3、4区依次递增（△）   根据当年IF 根据3年平均IF   Q1影响力总和最大，Q2/Q3/Q4依次递减（▽） 1区、2区、3区、4区影响力总和均等   没有设置大类学科，只分为177（最新）个具体学科 将JCR中所有期刊13大类    Tips：
 JCR分区的Q1期刊很可能在中科院分区的2区； 中科院分区不仅仅根据13个大类进行排名，还会根据177（最新）个不同学科进行小类分区，通常说的都是大区，小区分区容易导致期刊分区高。  TOP期刊？ 最新版本的分区表里，大类的1区期刊默认为top期刊，大类的2区期刊中2年总被引频次位于前10%的期刊，经同行评议认定TOP期刊；小类1区期刊设置为top期刊；不在上述范围内的经过科学共同体评议，被认为是高学术影响力的期刊，也被采纳加入top期刊。
参考： SCI的两三事：JCR分区和中科院分区 | SCI分区，该看JCR还是中科院?</description>
    </item>
    
    <item>
      <title>鼠年寒假遇上新型冠状病毒</title>
      <link>https://aileenxie.github.io/2020/13ncov/</link>
      <pubDate>Wed, 29 Jan 2020 20:08:41 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2020/13ncov/</guid>
      <description>现在是2020鼠年的寒假
今年的寒假遇上了新型冠状病毒2019_nCoV
武汉真的太惨了
最早一例从12月初就出现了
然而到我回家18号左右才慢慢引起了关注
医用外科口罩、N95口罩全部脱销，网上线下都买不到
我23号买的口罩，说是顺丰不间断配送，到今天也没见到送
感染人数增速超快
好久没见到的小喇叭绕着居民区兜圈呼吁：“出门戴口罩，&amp;hellip;勤洗手，不要&amp;hellip;，避免去亲朋好友家聚会..”
最开始根本叫不到老爸老妈戴口罩，不带在意的。现在他们都开始囤粮食防封城了&amp;hellip;
目前确诊6000+， 超过SARS规模了。
全国性延后春节假期，上海更是红头文件禁止企业2月10号之前开工。
我的假期又长了一点
学校则是禁止提前返校，今年2月23开学，如果不去实习的话还有一个月的假期。
希望早日度过这个艰难的时刻，疫情早点得到控制，加油！
(数据摘自丁香医生)</description>
    </item>
    
    <item>
      <title>Matlab常用函数</title>
      <link>https://aileenxie.github.io/2019/12matlab/</link>
      <pubDate>Tue, 27 Aug 2019 11:01:18 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2019/12matlab/</guid>
      <description>总结一些Matlab常用or易忘的函数，更详细和全面的用法查阅官方文档：Mathworks Help。
计时 tic toc tic和toc是用来记录matlab命令执行的时间：tic用来保存当前时间，而后使用toc来记录程序完成时间。
tic // 任意表达式 toc t=toc  t存储所耗时间，不加t=toc这句，运行完这段代码也会输出运行时间：
Elapsed time is 0.000158 seconds.  相似的还有cputime（返回从调用该函数起所用的总的Cpu时间，单位以秒计算。）、etime（e=etime(t2,t1)命令返回向量t1和t2之间的时间段，t1和t2必须含有由clock函数返回的6个元素，即[Year Month Day Hour Minute Second]）。 矩阵&amp;amp;向量 重复：repmat B = repmat(A,m,n); % 重复矩阵/向量/值A为 行m个，列n个  返回下三角： tril B = tril(A,m,n); % 返回矩阵下三角部分，其余补0  维度重排：permute B = permute(A,order); % 设A是10x20x5的矩阵，B=permute(A,3,1,2)，则B为5x10x20的矩阵  拼接  横向拼接[a b]或[a,b]; 纵向拼接[a;b].  删除单一维度: squeeze B = squeeze(A); % 设A是10x20x1的矩阵，则B为10x20的矩阵  　向量对比：intersect, setdiff C=intersect(A,B); % 返回向量A,B中共有数据（经过排序），不包含重复项 C=setdiff(A,B); % 返回在A中存在，B中不存在的数据（经过排序），不包含重复项  　随机数 控制随机数种子：rng rng(seed); % 使用非负整数 seed 为随机数生成器提供种子，以使 rand、randi 和 randn 生成可预测的固定数字序列。默认seed为&#39;default&#39; rng(&#39;shuffle&#39;); % 根据当前时间为随机数生成器提供种子。这样，rand、randi 和 randn 会在您每次调用 rng 时生成不同的数字序列 s = rng; % 捕获当前种子，以便后续rng(s)还原  生成随机数矩阵：rand,randn,randi A = rand(m,n); % 生成(0,1)均匀分布的mxn矩阵，没有n时--&amp;gt;生成mxm矩阵 B = randn(m,n); % 生成标准正态分布的mxn矩阵，没有n时--&amp;gt;生成mxm矩阵 C = randi(imax,m,n); % 生成介于 1 和 imax 之间均匀分布的随机整数mxn矩阵，没有n时--&amp;gt;生成mxm矩阵  随机采样：randperm s = randperm(n,k); % n&amp;gt;k；返回从整数1到n的随机均匀采样的k个值。s是1xk的行向量。  随机采样：randsample 比randperm更高层次一点，对各功能进行了封装</description>
    </item>
    
    <item>
      <title>Matlab配置Xcode编译</title>
      <link>https://aileenxie.github.io/2019/11xcode/</link>
      <pubDate>Tue, 27 Aug 2019 08:50:51 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2019/11xcode/</guid>
      <description>Matlab使用时经常需要编译一些其他语言的文件来调用，这需要相应的编译器来完成编译过程。Mac上编译C/C++可以选用Xcode。
安装Xcode Appstores 就能直接下载安装Xcode
安装command line tools 在终端上输入：
xcode-select --install  然后会有弹窗提示安装，跟着向导一步步完成安装。
确认安装成功的方法是：打开Xcode -&amp;gt; New -&amp;gt; Project，看到Command line tools。 打开matlab项目，运行命令，看是否成功检测到编译器。
mex -setup  　　还不成功 ？ 可能需要进行的操作&amp;hellip;
我在Matlab R2018a和Xcode v10.1环境下，安装完Command line tools之后再运行mex -setup就能成功找到编译器了！但是我看其他人有碰到需要更改配置文件的步骤，我还是记录一下，以便以后遇到问题多一条思路。具体方法是替换Matlab的启动和配置文件mexopts.sh和clang_maci64.xml里MacOSX版本号。
查看本机MacOSX SDK版本号  在/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs中查看自己的版本号：  或者终端运行命令
xcrun -sdk macosx --show-sdk-path  或
xcrun -sdk macosx --show-sdk-version   更改mexopts.sh文件  打开Matlab，命令行运行
edit ([matlabroot &#39;/bin/mexopts.sh&#39;])  打开mexopts.sh文件，以防万一提前备份一下该文件。把该文件中所有macosx10.x更改为本机MacOS版本号。
 让更改生效：
cd (matlabroot) cd bin mex -setup  输入1，然后回车，输入y，回车。 详见CSDN博文</description>
    </item>
    
    <item>
      <title>Matlab下Libsvm安装使用</title>
      <link>https://aileenxie.github.io/2019/10libsvm/</link>
      <pubDate>Mon, 26 Aug 2019 23:34:14 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2019/10libsvm/</guid>
      <description>下载 从github上拉取源码到你想放的目录（我放在matlab工作目录/toolbox/），源码地址：GitHub - cjlin1/libsvm。
编译 文件windows/目录下有预先编译好的libsvmread.mexw64、libsvmwrite.mexw64、svmtrain.mexw64、svmpredict.mexw64文件。如果是windows 64位下的matlab可以直接使用这些编译文件。
我的是mac版的matlab,需要重新编译相应的.mexmaci64文件，windows32的则是编译出.mexw32文件。具体来说：
 先确保电脑装了C/C++编译器，我电脑本身装了Xcode自带Clang，不需要再装编译器了，没装的随便装个编译器再进行下面的步骤（参考Matlab配置Xcode编译）。
 在Matlab中进入Libsvm根目录下的matlab目录（如:toolbox/libsvm-3.23/matlab），在命令窗口输入
mex –setup  Matlab会提示你选择编译mex文件的C/C++编译器，就选择一个已安装的编译器，如Xcode with Clang。之后Matlab会提示确认选择的编译器，输入y进行确认。  输入以下命令进行编译:
make  编译成功后，当前目录下会出现4个后缀为mexmaci64的文件。（tips:Matlab或VC版本过低可能会导致编译失败，建议使用最新的版本）。
 把编译出的这4个文件加入matlab的路径以便之后的调用。
  重命名 编译好后，由于产生的文件svmtrain和svmpredict与Matlab中自带有SVM的工具箱中的函数同名，为了避免调用不了，可以把这两个文件的编译文件重命名一下。我是重命名成了libsvmtrain和libsvmpredict，然后用新的名字来调用函数即可。
基本使用 libsvmread函数 用来读取以LIBSVM格式存储的数据文件。libsvm包根目录里有个示例数据文件heart-scale。
[label_vector, instance_matrix] = libsvmread(&amp;quot;heart-scale.txt&amp;quot;);  函数输出：label_vector是数据标签，instance_matrix是数据矩阵。
libsvmwrite函数 用来把matlab数据矩阵和标签存成LIBSVM格式数据文件data.txt:
libsvmwrite(&amp;quot;data.txt&amp;quot;, label_vector, instance_matrix]  libsvmtrain函数 用来训练SVM分类器模型，具体参数如下：
model = svmtrain(label,inst,Parameters)  其中：model.Paramaters = [-s,-t,-d,-g,-r]
 【输入】数据标签+数据矩阵+Parameters: 一个5 x 1的向量，从上到下依次表示：
　-s SVM类型（默认0）:
　0 &amp;ndash; C-SVC,
　1 &amp;ndash; nu-SVC,</description>
    </item>
    
    <item>
      <title>研一总结</title>
      <link>https://aileenxie.github.io/2019/09essay/</link>
      <pubDate>Sat, 24 Aug 2019 12:56:56 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2019/09essay/</guid>
      <description>忙完了论文，在暑假的最后两周才真正开始了暑期模式。
研一这一年上完了所有课程从0开始完成了一篇论文并投稿，成长挺大的。博客空了很久没更，趁这两周把之前的零碎知识点整理一下放上博客。这篇作为研一一年的学习记录。
课程 除了统修的政治课（中特社、辩证法）和英语课，专业相关课程及结课作业统计如下：
 【算法设计与分析】：考试 【大规模非线性规划】：论文汇报Faster Derivative-Free Stochastic Algorithm for Shared Memory Machines新的异步随机零阶算法（AsySZO+）； 【图形图像处理技术】：论文汇报Taskonomy: Disentangling Task Transfer Learning； 【数据管理系统实现】：论文汇报Impala: A Modern, Open-Source SQL Engine for Hadoop，论文复现Adaptive Cardinality Estimation自适应查询优化aqo；
 【机器学习】：论文汇报Snap ML: A Hierarchical Framework for Machine Learning；
 【专业外语】：论文汇报——密码学新方向；
 【云计算安全】：论文汇报Multi-Kernel, Deep Neural Network and Hybrid Models for Privacy Preserving Machine Learning用于隐私保护机器学习的多核，深层神经网络和混合模型；
 【计算机视觉】：深度估计方向论文总结，图像检测方向综述，论文复现 RetinaNet。
  论文 论文研究的是传统机器学习的特征工程 &amp;amp; 优化问题，记录一下整个过程。
 最初参考的文献是以下四篇:①Statistical Estimation and Testing via the Sorted l1 Norm, ②Group SLOPE - adaptive selection of groups of predictors, ③Discriminative Structured Feature Engineering for Macroscale Brain Connectomes, ④Support Matrix Machines。涉及到统计学的假设检验、多重假设检验矫正（BHq过程），这些知识点我在之前的博客有整理过。前两篇论文则是借鉴这些统计学方法，提出了针对线性回归的新的机器学习方法。后两篇则是针对矩阵数据的结构性特征工程方法。在反复研读这几篇文章之后，我对其中的思路和推导都有一定了解后，又陆续看了更多的相关论文，加深认识，进而提出了新的方法，并对该方法的解析解进行了推导。</description>
    </item>
    
    <item>
      <title>AI线稿上色——Style2paints</title>
      <link>https://aileenxie.github.io/2019/08style2paints/</link>
      <pubDate>Thu, 31 Jan 2019 21:03:40 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2019/08style2paints/</guid>
      <description>快期末的时候喜得一个wacon的板子，很久没画画了，业务生疏还有点不习惯，画得很慢。偶然被推荐一个很有意思的开源项目Style2paints（源码地址），一个AI线稿上色工具，可以快速地进行线稿的智能上色。
但是很不巧，1月18号Style2paints-V4版本的服务器就因为经费原因关闭了，我看到这个项目的时候已经无法在线访问了，只好尝试本地搭环境启动。目前代码只开源了V3版本，看官方描述V3和V4差别还蛮大的。V4是模拟人工上色流程对线稿进行分层上色，输出也是PSD分层的文件，V3输出的是单层的图片，操作也不太一样。
V3页面（自截图）： V4页面（官方介绍视频里截的）：
这里记录一下mac上Style2paints-V3本地搭建的过程。
1. Python环境 首先要准备python3的环境，mac下的安装可以参考之前的文章，用homebrew直接安装。
2. 依赖包 这里没有用gpu版本的tensorflow，安装以下依赖包：
pip3 install tensorflow (安装需要翻墙，全局模式命令行安装)
pip3 install keras
pip3 install bottle
pip3 install gevent
pip3 install h5py
pip3 install opencv-python
pip3 install scikit-image
pip3 install paste
3.源码运行 拉取源码：
git clone https://github.com/lllyasviel/style2paints  到V3目录下执行server/server.py文件。正常运行的话可以直接本地打开 http://127.0.0.1:80/ 就能看到界面。但是我运行的过程中有以下几点异常。
4. Q:Tensorflow编译警告  Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX
原因是CPU支持AVX扩展(高级矢量扩展)，但是默认安装（pip install）的TensorFlow版本不支持使用扩展。其实用了扩展CPU也比GPU慢得多。解决办法有两个：
1）忽略警告，直接在server.py代码中加入：
import os os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;  2)源码编译tensorflow 从针对CPU优化的源去构建tensorflow，不仅可以去掉这个警告，还能提高CPU下的tensorflow的性能。具体参考tensorflow的issue8037</description>
    </item>
    
    <item>
      <title>论文书写工具准备——MacTeX</title>
      <link>https://aileenxie.github.io/2018/07use_mactex/</link>
      <pubDate>Mon, 19 Nov 2018 20:00:38 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/07use_mactex/</guid>
      <description>前言  TEX：斯坦福大学的教授Donald E.Knuth开发的一个功能强大的幕后排版系统。它是一种语言，类似于Java和C之类的计算机语言，但是它是为简单的排版操作设计的； LaTeX：是TEX的众多宏集之一，是由Leslie Lamport编写的。将一些常用到的功能整合为文档类型中的设置，简化了TEX排版的工作量及难度； CTEX：利用TEX排版系统的CTEX中文套装的简称。它集成了编辑器 、WinEdt和 PostScript处理软件 Ghostscript 和 GSview 等主要工具。 TeXLiv：是一个TeX发行版，它是一组程序的集合，主要作用就是将你写的TeX代码进行解析排版输出成PS或者PDF。“TeX发行版相对于TeX语言”大致可以理解为“C语言编译器(如GCC或Clang)相对于C语言”的关系； MacTeX：是Mac版的TeXLive。  本文MacTeX搭配Sublime(编辑器)和Skim(PDF阅读器)食用。
MacTeX安装 官网：http://tug.org/mactex/ 安装好后会看到一串东西（我把它们放到了同一个文件夹里）： TeXShop：它是一个官配的编辑器，比较简单。我们这里用Sublime取代。
Sublime和Skim安装 Sublime官网：https://www.sublimetext.com/ Skim官网：https://skim-app.sourceforge.io/ 图标长这样： 环境配置 Sublime配置 打开Sublime，我们需要安装LaTeX相关插件才能更好地使用，插件从Package Control官网下载，步骤如下： 1. 进入Sublime的终端：ctrl+ [esc下面那个顿号] 或 View &amp;gt; Show Consoles 2. 从Package Control官网复制灰色代码段输入Sublime的终端并回车运行，注意对应的版本号： 3. 等待安装完成后，退出重启Sublime; 4. 安装插件包：Command+Shift+P或Tools &amp;gt; Command Palette…&amp;mdash;&amp;gt;输入&amp;rdquo;install package&amp;rdquo;回车&amp;mdash;&amp;gt;输入“LaTeX Tools”，找到这一项并点击安装。完成后重启Sublime。 5. 新建一个文件test.tex(一定要有后缀.tex!!!)，复制以下内容进去：
\documentclass{article} \begin{document} Hello LaTeX \end{document}  可看到Latex格式文件有彩色样式，完成！
Skim配置 打开Skim后，Command+,或Skim &amp;gt; 选项（performance）&amp;mdash;&amp;gt;点到【同步（sync）】页&amp;mdash;&amp;gt;设置Preset为Sublime(不知道为什么我这边没有Sublime3选项，选了2也可行) 测试环境 回到Sublime刚才新建的test.tex文件，Command+B或Tools &amp;gt; Build编译tex文件，顺利的话，Skim会自动弹出PDF文件预览窗口。环境搭建成功！
至此整个环境就搭完了，具体怎么用mactex写出漂亮的文章和公式参考LaTeX快速入门：一文浅谈TeX排版语法，还有些小技巧我会之后总结一篇文章。待续。
参考文章1 参考文章2</description>
    </item>
    
    <item>
      <title>Mac下安装Python3</title>
      <link>https://aileenxie.github.io/2018/06python_mac/</link>
      <pubDate>Mon, 19 Nov 2018 20:00:13 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/06python_mac/</guid>
      <description> -- 简单记录mac下Python3安装步骤。
安装Python3  应先安装C编译器。最快的方式是运行
xcode-select --install  来安装Xcode命令行工具。
 安装一个包管理工具Homebrew
$ /usr/bin/ruby -e &amp;quot; $ (curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;  插入PATH（在~/.profile文件末尾加）
export PATH=/usr/local/bin:/usr/local/sbin:$PATH  (source .profile 一下使之生效)
 安装Python3
brew install python  查看
python3 --version  确定pip是否安装
pip3 --version  若未安装
python3 -m ensurepip --default-pip  pip的使用详见相关参考文档
  </description>
    </item>
    
    <item>
      <title>FDR与BH过程</title>
      <link>https://aileenxie.github.io/2018/05fdr/</link>
      <pubDate>Mon, 19 Nov 2018 19:59:46 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/05fdr/</guid>
      <description>--  --  最近导师让看的paper里用到了FDR的理念，借这个机会把FDR相关概念理解了一遍，整理出来。
统计相关基础知识 假设检验 假设检验的基本思想是小概率反证法思想。反证法思想是先提出假设(检验假设H0)，在该假设下找到一个小概率事件（发生概率P&amp;lt;阈值，如0.01或0.05，即在一次试验中基本上不会发生）。如果事件发生，则可以拒绝这个假设（假设不成立），如果事件没有发生，则接受假设。
这么做的思路是：
 不轻易拒绝原假设：原假设即使真的成立，而观察的样本由于数量较少，观察值存在一定的波动。所以我们要给原假设一定范围的容忍度，这个容忍度要尽可能大，观察值出现在这个范围内都是可以容忍的。 小概率事件发生不正常：如果小概率事件还是发生了，那么就说明原假设有问题。  思考：
 只有拒绝原假设才是有意义的，假设检验的做法确保了被拒绝的原假设在极大可能上是伪假设。即确保了“拒绝”的准确性。所以如果我想证明一个人是天才，那我要假设他是个凡人，然后找一个凡人几乎做不到的事情，如果他做到了，那么我就拒绝他是个凡人，从而证明他是个天才（见下面🌰）。 没有被拒绝的原假设，其实是在相当包容的情况下接受的假设，本身并没有多大意义  P值 即概率，反映某一事件发生的可能性大小。统计学根据显著性检验方法（t检验，t分布，卡方分布，F分布）所得到的P 值，一般以 P&amp;lt;0.05 为有统计学差异， P&amp;lt;0.01 为有显著统计学差异，P&amp;lt;0.001 为有极其显著的统计学差异。其含义是样本间的差异由抽样误差所致的概率小于0.05 、0.01、0.001。实际上，P值不能赋予数据任何重要性，只能说明某事件发生的几率。
给定显著性水准alpha时，可得出对应的拒绝域；根据当前试验，可以计算出P值。当P值越小时，表示此时试验得到的统计量t越落在拒绝域。因此基于P值的结果等价于基于t值的结果。因此，P值越小，拒绝原假设的信心越大。 原假设(H0) &amp;amp; 备择假设(H1)  原假设（又叫虚无假设、零假设）：一般是希望能证明为错误的假设 备择假设（又叫备选假设、对立假设）：希望被证明是正确的假设  一类错误 &amp;amp; 二类错误  第一类错误（假阳性）又称“Ⅰ型错误”、“拒真错误”，是指拒绝了实际上成立的、正确的假设，为“弃真”的错误，其概率通常用α（显著性水平）表示。 第二类错误（假阴性）又称“口错误”、“纳伪错误”、“第Ⅱ型错误”。假设检验术语。与“第一类错误”相对。在零假设H0本来不真的情况下，检验统计量的观测值落入接受域而接受Ho而犯的错误。用字母β表示。   🌰举个栗子 为了帮助理解，我编了个例子。
【例子一】小白五感超群，他自称能够辨别一杯白咖啡是先倒入的牛奶还是先倒入的咖啡。为了验证这个说法，我们进行一组实验。 实验内容：放5杯咖啡，让小白品尝并说出每一杯咖啡是先加奶还是先加咖啡。靠猜答对5杯咖啡的概率\(P=(\frac{1}{2})^{5}\approx 0.031=3.1\%\)。假设显著性水平（阈值）设为5%：
 H0：小白辨别咖啡是靠猜的 H1：小白辨别咖啡不是靠猜的（即小白有能力辨别）  试验结果是5杯咖啡都被成功辨别。此事件P = 3.1% &amp;lt; 5%，故可以拒绝原假设，承认小白能够辨别白咖啡。但有3.1%的可能判断错误（I类错误），因为有3.1%的可能小白就是靠猜的。可以通过增加咖啡杯数来降低P值（靠猜全答对的难度越来越高），这样拒绝原假设的自信度就越高。
多重假设检验校正 多次检验导致的大量假阳性：如果检验一次，犯错的概率是5%；检验10000次，犯错的次数就是500次，即额外多出了500次差异的结论（即 使实际没有差异）。
结合例子🌰说就是：假设一人品咖啡靠猜对概率是5%，10000个人来品咖啡就会有500个人靠猜来答对，如果阈值还设为5%的话，这500个人都被判为是有能力辨别咖啡能力的人，但是明明他们都是靠猜的啊！！！那么这就有500个一类错误。
所以当同一个数据集有n次（n&amp;gt;=2）假设检验时，要做多重假设检验校正。这里讨论Bonferroni校正和FDR校正两种P值校正方法。
Bonferroni —— “最简单严厉的方法” 如果检验1000次，我们就将阈值设定为5%/1000=0.005%；即使检验1000次，犯错误的概率还是保持在0.005%×1000 = 5%。最终使得预期犯错误的次数不到1次，抹杀了一切假阳性的概率。 该方法虽然简单，但是检验过于严格，导致最后找不到显著表达的蛋白（假阴性）。</description>
    </item>
    
    <item>
      <title>t分布, 卡方x分布，F分布</title>
      <link>https://aileenxie.github.io/2018/04txf/</link>
      <pubDate>Mon, 22 Oct 2018 15:07:13 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/04txf/</guid>
      <description>--  --  --  T分布：温良宽厚 本文转自“医学统计分析精粹”，小编“Hiu”原创完成， 原文链接 参考文章
命名与源起 “t”，是伟大的Fisher为之取的名字。Fisher最早将这一分布命名为“Student&amp;rsquo;s distribution”，并以“t”为之标记。
Student，则是William Sealy Gosset（戈塞特）的笔名。他当年在爱尔兰都柏林的一家酒厂工作，设计了一种后来被称为t检验的方法来评价酒的质量。因为行业机密，酒厂不允许他的工作内容外泄，所以当他后来将其发表到至今仍十分著名的一本杂志《Biometrika》时，就署了student的笔名。所以现在很多人知道student，知道t，却不知道Gosset。（相对而言，我们常说的正态分布，在国外更多的被称为高斯分布……高斯泉下有知的话，说不定会打出V字手势~欧耶！）
看懂概率密度图 这一点对于初学者尤为重要，相信还是有不少人对正态分布或者t分布的曲线没有确切的理解。
首先，我们看一下频率分布直方图，histogram： 上图，最关键的就是横轴了，柱高，即，对于横轴上每一个点，发生的频次。图中横轴为4对应次数最多，大约12次；依次类推，横坐标为10处，发生1次……
我们做单变量的探索性数据分析，最喜欢做柱状图了，或者再额外绘制一条Density曲线于其上（见下图）。很容易就可以看出数据的分布（集中趋势. 离散趋势），图中，数据大多集中在4左右（均数. 众数），有一点点右偏态，但基本还是正态分布。
下图，手绘曲线，即密度曲线，英文全称Probability Density Function/Curve。实际上是对上面柱状图的一个平滑，但它的纵坐标变为了概率，区别于柱状图的频次。但理解起来意义差不多。 以下，我们就用Density曲线来讲解T分布的特征。
T分布的可视化 我们平常说的t分布，都是指小样本的分布。但其实正态分布，可以算作t分布的特例。也就是说，t分布，在大小样本中都是通用的。
之前有读者问过：“是不是样本量大于30或者大于50，就不能用t分布了呀”？
完全不是这样的！t分布，大小通吃！具体且看下文分解。
相对于正态分布，t分布额外多了一个参数，自由度。自由度 \(\nu = n - 1\)。我们先看几个例子，主观感受一下t分布。
可见，随着样本量n(自由度\(\nu\))的增加，t分布越来越接近正态分布。正态分布，可以看做只是t分布的一个特例而已。
以上部分大家大概都学过的，相信大多数读者都会了解。但这里，让我们回到我们的标题（不是标题党）：温良宽厚。
大家仔细比较一下下图。t分布（红色）虽然也是钟型曲线，但是中间较低. 两侧尾巴却很高。
这就是t分布的优势！这个特征相当重要，百年来，t分布就指着这个特征活着的！
比较一下上图两条曲线，我用这样一个词，“宽厚”，来形容t分布曲线的特征。是不是比正态分布曲线更宽啊？是不是比正态分布曲线更厚呢？
大家都说重要的事要重复三遍，我们再重复一下，样本量越小（自由度越小），t分布的尾部越高。
尾部的高度，有十分重要的统计学意义。 我们来比较一下下图中的两条曲线。这两条曲线同样都是对图中底部6个黑色点（数值）进行分布拟合。
 我们首先看一下那条矮的. 正态分布的曲线。我们前面说过，正态分布的曲线不具备“宽厚”的特征。它的尾部很低，尾部与横轴之间高度很“狭窄”。也就是说，正态分布不能够容忍它长长的尾部出现大概率的事件（图中横轴值为15处一圆点出现概率为六分之一），所以正态分布就很无奈地，将这一点纳入它的胸膛而非留在尾部。于是乎，恶果就出现了：图中正态分布的均数，远远偏离了大多数点所在的位置，标准差也极大。总之，与我们所期待的很不一致。   再看一下那条高高的t分布曲线。我们前面说过了，t分布“温良宽厚”，它的尾巴很高（本图中不明显，参见上面自由度为1,2,3时所对应的图片），高高的长尾让它有“容人的雅量”。所以，这条t分布的曲线，很好的捕捉到了数据点的集中趋势（横坐标：0附近）和离散趋势（标准差：只是那条正态分布曲线标准差的四分之一）。  这也是T分布盛行的原因，即T分布被广泛应用于小样本假设检验的原因。虽然是很小的样本，但是，却强大到可以轻松的排除异常值的干扰，准确把握住数据的特征（集中趋势和离散趋势）！
准确捕捉变量的集中趋势和离散趋势在统计中有极为重要的意义，几句话难以说清，简单举几个栗子：
 研究样本量的估计量更小。熟悉样本量计算的朋友也知道，标准差是样本量计算的一个重要参数。上例中，我们t分布的标准差只是正态分布的四分之一，那么我们计算所需的样本量也会极大的减少（只需原来的16分之一），极大地降低研究经费和工作量！（关注“医学统计分析精粹”，回复关键词“样本量”，可以看到很handy的样本量计算工具哦！）
 我们缩小了标准差，熟悉假设检验（将在后续“看图说话”系列文章中出现）的朋友也不难看出，如此，我们更容易得到一个有意义的P值！
 点估计更准确。如果我们需要根据一个小样本数据来估计学生的平均身高。那么使用正态分布来拟合，很容易就受到离群异常值的影响而得到错误的估计。
 回归中应用t分布，可以得到更稳健的估计量（β值或OR值），这也是我们实现“稳健回归”的一个重要手段。
  通过下面一幅图，我们巩固一下t分布的“宽厚”： 与正态分布曲线（矮胖）比较，t分布以其高高的尾部（本图中不明显，参见上面自由度为1,2,3时所对应的图片），容忍了在横轴为9处的异常值，得到了更稳健的集中趋势估计值（均值1.11）和更紧凑的离散趋势估计值（标准差差0.15，又是正态分布的四分之一）。要知道，我们如果单单想通过增加样本量来将标准误（假设检验中使用的参数，标准差除以自由度的平方根）缩减到四分之一，需要16倍的样本量！可见，t分布当真是威力无穷！
PS：上述两幅图中的t分布曲线并不是频率学派应用t分布的常规套路（更像是贝叶斯学派的用法）。细心者可以发现，我们使用的t分布的自由度明显低于n - 1的自由度计算方法。这里的自由度是根据最大似然法估计出来的，用以更恰当地拟合数据的分布。虽然这与我们平时的用法不同，但小编觉得，这一点点不同不仅无伤大雅，反而更有利于大家深入理解t分布的特征——温良宽厚。</description>
    </item>
    
    <item>
      <title>近端梯度（Proximal Gradient, PG）算法详解</title>
      <link>https://aileenxie.github.io/2018/03pg/</link>
      <pubDate>Fri, 19 Oct 2018 15:07:13 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/03pg/</guid>
      <description>--  --  --  PG算法 简介 Proximal算法是用于求解凸优化问题的方法之一。用于求解目标函数形如f(x)=g(x)+h(x)（其中g(x)可微而h(x)不可微）形式无约束问题的下降算法。
 当无约束的凸优化问题可微，我们可以用梯度下降算法求解； 当无约束的凸优化目标函数不可微，我们可以采用次梯度(subgradient)算法求解(该算法时间复杂度较高,且不会产生稀疏)； 当存在约束时，我们可以采用proximal相关梯度算法求解(可降低复杂度至O(1/ϵ));  这是因为，当目标函数存在约束时，我们可以把约束写入目标函数，但是这时候往往目标函数就从可微变成不可微，例如线性回归加入L−1norm，记为\( ∥y−xβ∥_2^2+λ∥β∥_1\) ，那么很明显，目标函数前半部分为凸且连续可微，但后半部分为凸但不连续可微。
算法模型 其中g(x)为凸函数，可微。h(x)也是凸函数，但不可微。其中g(x),h(x)是由F(x) 分离出来的两项，当F(x) 分离的结果不同，即使是同一个问题，算法的实现方式也不尽相同，
基本概念 【临近算子】（proximity operator） &amp;gt; 当h(x)=0,则prox\(h\)(x)=\(arg \min _{u}\)(\(\frac{1}{2}||u−x||_2^2\))=x; &amp;gt; 当h(x)=Ic，则prox\(h\)(x)=\(arg \min _{u∈c}\)(\(\frac{1}{2}||u−x||_2^2\))=Pc(x); &amp;gt; 当h(x)=\(t||X||_1\)，则prox\(h(x)\) 为软阈值算法。 临近算子是对梯度的延伸，只与h(x)有关，当函数h(x)为光滑函数时，该临近算子就是梯度。
F(x)的【近端梯度】（proximal gradient） 近端梯度就是对F(x)梯度的近似，其中： 集合X的【指示函数】（indicator function） 其中X是一个凸集合。利用指示函数，我们可以将有约束问题写成无约束问题,如下：
当x不在X中，等式为无穷大，因此x肯定不是最优值。因此就等于限定了x在凸集合X中。
变量x在集合X上的【投影算子】（projection operator） 投影的含义：一个点x在集合X上的投影，就是X上离x的欧几里得距离最近的点。
Tips:
 2个在集合外的点x，y之间的距离，一定大于这两点在凸集合X上的投影的距离 当h(x)=\(l_{X}(x)\)时，\(proj_x(x)=prox_h(x)\)。  算法实现 迭代计算最优解x，直到F(x)在最小值附近收敛。
算法伪代码： 资料：
Proximal Algorithms
Projected Gradient Method and LASSO 
CSDN-Proximal Gradient Method近端梯度算法
近端梯度法(Proximal Gradient Method, PG)</description>
    </item>
    
    <item>
      <title>L0,L1,L2以及核范数规则化</title>
      <link>https://aileenxie.github.io/2018/02l_norm/</link>
      <pubDate>Wed, 10 Oct 2018 19:17:39 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/02l_norm/</guid>
      <description>看到一篇讲解很详细的文章，边看边做笔记整理要点。 先放一张范数汇总图： 前言 监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。
最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。
 过拟合：参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力。 泛化能力强：测试误差也很小 范数规则化有两个作用：
1）保证模型尽可能的简单，避免过拟合。
2）约束模型特性，加入一些先验知识，例如稀疏、低秩等。
  目标函数 一般来说，监督学习可以看做最小化下面的目标函数：  第一项L(yi,f(xi;w)): 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。
 如果是Square loss,那就是最小二乘了； 如果是Hinge Loss，那就是著名的SVM了； 如果是exp-Loss，那就是牛逼的 Boosting了； 如果是log-Loss，那就是Logistic Regression了； 不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。
  第二项λΩ(w): 也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。
 本文讨论的即是“规则项Ω(w)”； 一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数； 论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等；    L0范数与L1范数 L0范数 L0范数（||W||0）是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。
L1范数 L1范数（||W||1）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。L1范数会使权值稀疏，它是L0范数的最优凸近似。
 Tips: 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微。  为什么不用L0，而用L1？  是因为L0范数很难优化求解（NP难问题）； 是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。 总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。  为什么要稀疏？  特征选择(Feature Selection)：去掉没有信息的特征，及将对应权重置0，防止无用特征对测试新样本的干扰； 可解释性(Interpretability)：如最初有1000个特征，回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b，通过学习，如果最后学习到只有5个非零的wi，那么就可以说影响患病率的主要就是这5个特征，医生就好分析多了。  L2范数 L2范数（||W||2）是指向量各元素的平方和然后求平方根。它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”（weight decay）它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。与L1范数不同，它不会让它等于0，而是接近于0。</description>
    </item>
    
    <item>
      <title>Hugo快速搭建博客</title>
      <link>https://aileenxie.github.io/2018/01hugoblog/</link>
      <pubDate>Fri, 05 Oct 2018 21:52:20 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/01hugoblog/</guid>
      <description>简单记录一下mac下用Hugo搭建博客的过程，以便日后查阅。感谢renyijiu的安利和帮助。
Hugo  一个用Go语言写的静态网站生成器 能把markdown转变成静态网页 内置web服务期，便于本地草稿调试  安装 Hugo  直接用Homebrew安装
brew install hugo  完成之后查看版本，我目前装的是0.49
hugo version  创建一个新站点 到你的目录下找个喜欢的地方执行语句
hugo new site myblog   会在当前目录生成一个叫“myblog”的目录，里面包含若干文件夹和一个config.toml文件:
▸ archetypes/
▸ content/ -&amp;gt;你写的markdown文章
▸ layouts/ -&amp;gt;网站的模板文件
▸ static/ -&amp;gt;放的是一些图片、css、js等资源
▸ data/
▸ themes/ -&amp;gt;放的是你之后添加的主题
config.toml -&amp;gt;网站的配置文件
添加主题  进入目录页
cd myblog  初始化为git项目，方便之后放入github或者任何仓库进行版本管理
git init  添加主题, 以LeaveIt为例
git submodule add https://github.com/liuzc/LeaveIt.git themes/LeaveIt  之后会在themes/目录下看到你添加的主题。
 修改配置文件(直接打开config.toml文件修改也是一样的)
echo &#39;theme = &amp;quot;LeaveIt&amp;quot;&#39; &amp;gt;&amp;gt; config.</description>
    </item>
    
    <item>
      <title>New Start</title>
      <link>https://aileenxie.github.io/2018/00first-post/</link>
      <pubDate>Fri, 05 Oct 2018 13:01:24 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/00first-post/</guid>
      <description>重新回到学校一个月，第一个国庆避开人山人海，在上海安安心心休息了几天。
之前工作的时候一直想要搭一个博客，把每天零零碎碎记录的笔记整理出来放在一起，但一直没能着手开始行动（大概是又忙又懒。。）直到笔记随手记了一大堆，越来越不想整理。
回到学校自由时间多了很多，又在学习新东西，想着这是个好的时机开始写博客，终于在这个国庆开始了这项活动。
关于笔记  最开始一年我用的ubuntu系统，被安利了cherrytree，试用了一下，马上被这种可以“遍地开花”随手快速记录的树状结构笔记软件所吸引。 那年我刚工作，又是从通信跨到互联网行业，很多东西对我来说都是新知识：数据库(Mysql/Postgres)、编程语言（JAVA/Groov/Python)、测试框架（Spock/Selenium/Appium/rf/jmeter)、版本管理系统（Git）、持续集成（Gradle/Jenkins）包括Linux系统本身&amp;hellip; 用cherrytree记了相当多的笔记，这一年也是我收获颇丰的一年。 第二年换了macOS系统，cherrytree用不了（用模拟器打开页面很丑），先后换了有道云笔记，为知笔记，最后定格在了bootsnote（页面简洁好看，支持markdown和snippet）。又陆续做了关于ruby系测试框架（Cucumber/Capybara）、mac系统等的笔记。直到开学，我作为测试开发的工作告一段落。笔记也就此暂停。  关于博客  终于展开新的研究生生活，投身于机器学习、深度学习和计算机视觉方向研究。有种可以大展拳脚的感觉略兴奋。从开学前到现在，看过了CS229吴恩达的机器学习基础课程，CS231n斯坦福卷积神经网络课程，学了pytorch框架，开始看导师给的paper。最初看的很细致，还做了手写的笔记，现在想来还是要有个地方集中放置一下，就从现在开始重新规划一下之后的笔记。预计会放一些paper的阅读笔记，学习的知识点，或许是任何想记下来的东西吧hhh&amp;hellip;  七天假期马上要过去啦，搭好了博客算是一个很好的开始，希望能我好好充实这个博客，记录整个研究生阶段以及未来和以后更远更远的学习旅程，(๑•̀ㅂ•́)و✧加油！！</description>
    </item>
    
  </channel>
</rss>