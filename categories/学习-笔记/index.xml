<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>学习 · 笔记 on Aileen&#39;s Blog</title>
    <link>https://aileenxie.github.io/categories/%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 学习 · 笔记 on Aileen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch-cn</language>
    <lastBuildDate>Mon, 19 Nov 2018 19:59:46 +0800</lastBuildDate>
    
	<atom:link href="https://aileenxie.github.io/categories/%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>FDR与BH过程</title>
      <link>https://aileenxie.github.io/2018/05fdr/</link>
      <pubDate>Mon, 19 Nov 2018 19:59:46 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/05fdr/</guid>
      <description>--  最近导师让看的paper里用到了FDR的理念，借这个机会把FDR相关概念理解了一遍，整理出来。
统计相关基础知识 假设检验 假设检验的基本思想是小概率反证法思想。小概率思想是指小概率事件（P&amp;lt;0.01或P&amp;lt;0.05）在一次试验中基本上不会发生。反证法思想是先提出假设(检验假设H0)，再用适当的统计方法确定假设成立的可能性大小，如可能性小，则认为假设不成立，若可能性大，则还不能认为假设不成立。
P值 即概率，反映某一事件发生的可能性大小。统计学根据显著性检验方法（t检验，Z检验，卡方检验，F检验等等）所得到的P 值，一般以 P&amp;lt;0.05 为有统计学差异， P&amp;lt;0.01 为有显著统计学差异，P&amp;lt;0.001 为有极其显著的统计学差异。其含义是样本间的差异由抽样误差所致的概率小于0.05 、0.01、0.001。实际上，P值不能赋予数据任何重要性，只能说明某事件发生的几率。
给定显著性水准alpha时，可得出对应的拒绝域；根据当前试验，可以计算出P值。当P值越小时，表示此时试验得到的统计量t越落在拒绝域。因此基于P值的结果等价于基于t值的结果。因此，P值越小，拒绝原假设的信心越大。 原假设(H0) &amp;amp; 备择假设(H1)  原假设（又叫零假设）是在一次试验中有绝对优势出现的事件 备择假设（又叫备选假设、对立假设）在一次试验中不易发生(或几乎不可能发生)的事件。  因此，在进行单侧检验时，最好把原假设取为预想结果的反面，即把希望证明的命题放在备择假设上。
一类错误 &amp;amp; 二类错误  第一类错误（假阳性）又称“Ⅰ型错误”、“拒真错误”，是指拒绝了实际上成立的、正确的假设，为“弃真”的错误，其概率通常用α（显著性水平）表示。 第二类错误（假阴性）又称“口错误”、“纳伪错误”、“第Ⅱ型错误”。假设检验术语。与“第一类错误”相对。在零假设H0本来不真的情况下，检验统计量的观测值落入接受域而接受Ho而犯的错误。用字母β表示。   🌰举个栗子 为了帮助理解，我编了个例子。
【例子一】小白五感超群，他自称能够辨别一杯白咖啡是先倒入的牛奶还是先倒入的咖啡。为了验证这个说法，我们进行一组实验。 实验内容：放5杯咖啡，让小白品尝并说出每一杯咖啡是先加奶还是先加咖啡。靠猜答对5杯咖啡的概率\(P=(\frac{1}{2})^{5}\approx 0.031=3.1\%\)。假设显著性水平（阈值）设为5%：
 H0：小白辨别咖啡是靠猜的 H1：小白辨别咖啡不是靠猜的（即小白有能力辨别）  试验结果是5杯咖啡都被成功辨别。此事件P = 3.1% &amp;lt; 5%，故可以拒绝原假设，承认小白能够辨别白咖啡。但有3.1%的可能判断错误（I类错误），因为有3.1%的可能小白就是靠猜的。可以通过增加咖啡杯数来降低P值（靠猜全答对的难度越来越高），这样拒绝原假设的自信度就越高。
多重假设检验校正 多次检验导致的大量假阳性：如果检验一次，犯错的概率是5%；检验10000次，犯错的次数就是500次，即额外多出了500次差异的结论（即 使实际没有差异）。
结合例子🌰说就是：假设一人品咖啡靠猜对概率是5%，10000个人来品咖啡就会有500个人靠猜来答对，如果阈值还设为5%的话，这500个人都被判为是有能力辨别咖啡能力的人，但是明明他们都是靠猜的啊！！！那么这就有500个一类错误。
所以当同一个数据集有n次（n&amp;gt;=2）假设检验时，要做多重假设检验校正。这里讨论Bonferroni校正和FDR校正两种P值校正方法。
Bonferroni —— “最简单严厉的方法” 如果检验1000次，我们就将阈值设定为5%/1000=0.005%；即使检验1000次，犯错误的概率还是保持在0.005%×1000 = 5%。最终使得预期犯错误的次数不到1次，抹杀了一切假阳性的概率。 该方法虽然简单，但是检验过于严格，导致最后找不到显著表达的蛋白（假阴性）。
结合例子🌰说就是：10000人品咖啡应当相应地降低阈值，设成5%/10000=0.0005%（相对应的，增加咖啡杯数，比如让一个人分辨15杯咖啡，这用猜的就很难全对了吧！！！），那么这10000个人只会有5个人能靠猜来答对，大大降低了伪能力者（I类错误）。但是！！！15杯咖啡让真正有辨别能力的小白来喝，也会有味觉疲劳、品错的时候啊。这时候就出现了II类错误（假阴性）——判别小白没有鉴别能力，是靠猜的&amp;hellip;
FDR —— “比较温和的方法校正P值” 假阳性错误控制法是Benjamini于1995年提出的一种方法。基本原理是通过控制FDR值来决定P值的值域。相对Bonferroni来说，FDR用比较温和的方法对p值进行了校正。其试图在假阳性和假阴性间达到平衡，将假/真阳性比例控制到一定范围之内。例如，如果检验1000次，我们设定的阈值为0.05（5%），那么无论我们进行多少次实验，这些实验结果出现假阳性的概率保持在5%之内，这就叫FDR＜5%。
Q值 衡量错误发现率FDR（false discovery rates）的指标。\(FDR=\frac{I型错误数}{总拒绝数} = \frac{V}{R}=Q,R=0时Q=0\)</description>
    </item>
    
    <item>
      <title>t分布, 卡方x分布，F分布</title>
      <link>https://aileenxie.github.io/2018/04txf/</link>
      <pubDate>Mon, 22 Oct 2018 15:07:13 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/04txf/</guid>
      <description>T分布：温良宽厚 本文转自“医学统计分析精粹”，小编“Hiu”原创完成， 原文链接 参考文章
命名与源起 “t”，是伟大的Fisher为之取的名字。Fisher最早将这一分布命名为“Student&amp;rsquo;s distribution”，并以“t”为之标记。
Student，则是William Sealy Gosset（戈塞特）的笔名。他当年在爱尔兰都柏林的一家酒厂工作，设计了一种后来被称为t检验的方法来评价酒的质量。因为行业机密，酒厂不允许他的工作内容外泄，所以当他后来将其发表到至今仍十分著名的一本杂志《Biometrika》时，就署了student的笔名。所以现在很多人知道student，知道t，却不知道Gosset。（相对而言，我们常说的正态分布，在国外更多的被称为高斯分布……高斯~泉下有知的话，说不定会打出V字手势~欧耶！）
看懂概率密度图 这一点对于初学者尤为重要，相信还是有不少人对正态分布或者t分布的曲线没有确切的理解。
首先，我们看一下频率分布直方图，histogram： 上图，最关键的就是横轴了，柱高，即，对于横轴上每一个点，发生的频次。图中横轴为4处，次数最多，大约12次；依次类推，横坐标为10处，发生1次……
我们做单变量的探索性数据分析，最喜欢做柱状图了，或者再额外绘制一条Density曲线于其上（见下图）。很容易就可以看出数据的分布（集中趋势. 离散趋势），图中，数据大多集中在4左右（均数. 众数），有一点点右偏态，但基本还是正态分布。
下图，手绘曲线，即密度曲线，英文全称Probability Density Function/Curve。实际上是对上面柱状图的一个平滑，但它的纵坐标变为了概率，区别于柱状图的频次。但理解起来意义差不多。 以下，我们就用Density曲线来讲解T分布的特征。
T分布的可视化 我们平常说的t分布，都是指小样本的分布。但其实正态分布，可以算作t分布的特例。也就是说，t分布，在大小样本中都是通用的。
之前有读者问过：“是不是样本量大于30或者大于50，就不能用t分布了呀”？
完全不是这样的！t分布，大小通吃！具体且看下文分解。
相对于正态分布，t分布额外多了一个参数，自由度。自由度 = n - 1。我们先看几个例子，主观感受一下t分布。
可见，随着样本量n / 自由度的增加，t分布越来越接近正态分布。正态分布，可以看做只是t分布的一个特例而已。
以上部分大家大概都学过的，相信大多数读者都会了解。但这里，让我们回到我们的标题（不是标题党）：温良宽厚。
大家仔细比较一下下图。t分布（红色）虽然也是钟型曲线，但是中间较低. 两侧尾巴却很高。
这就是t分布的优势！这个特征相当重要，百年来，t分布就指着这个特征活着的！
比较一下上图两条曲线，我用这样一个词，“宽厚”，来形容t分布曲线的特征。是不是比正态分布曲线更宽啊？是不是比正态分布曲线更厚呢？
大家都说重要的事要重复三遍，我们再重复一下，样本量越小（自由度越小），t分布的尾部越高。
尾部的高度，有十分重要的统计学意义。 我们来比较一下下图中的两条曲线。这两条曲线同样都是对图中底部6个黑色点（数值）进行分布拟合。
 我们首先看一下那条矮的. 正态分布的曲线。我们前面说过，正态分布的曲线不具备“宽厚”的特征。它的尾部很低，尾部与横轴之间高度很“狭窄”。也就是说，正态分布不能够容忍它长长的尾部出现大概率的事件（图中横轴值为15处一圆点出现概率为六分之一），所以正态分布就很无奈地，将这一点纳入它的胸膛而非留在尾部。于是乎，恶果就出现了：图中正态分布的均数，远远偏离了大多数点所在的位置，标准差也极大。总之，与我们所期待的很不一致。   再看一下那条高高的t分布曲线。我们前面说过了，t分布“温良宽厚”，它的尾巴很高（本图中不明显，参见上面自由度为1,2,3时所对应的图片），高高的长尾让它有“容人的雅量”。所以，这条t分布的曲线，很好的捕捉到了数据点的集中趋势（横坐标：0附近）和离散趋势（标准差：只是那条正态分布曲线标准差的四分之一）。  这也是T分布盛行的原因，即T分布被广泛应用于小样本假设检验的原因。虽然是很小的样本，但是，却强大到可以轻松的排除异常值的干扰，准确把握住数据的特征（集中趋势和离散趋势）！
准确捕捉变量的集中趋势和离散趋势在统计中有极为重要的意义，几句话难以说清，简单举几个栗子：
 研究样本量的估计量更小。熟悉样本量计算的朋友也知道，标准差是样本量计算的一个重要参数。上例中，我们t分布的标准差只是正态分布的四分之一，那么我们计算所需的样本量也会极大的减少（只需原来的16分之一），极大地降低研究经费和工作量！（关注“医学统计分析精粹”，回复关键词“样本量”，可以看到很handy的样本量计算工具哦！）
 我们缩小了标准差，熟悉假设检验（将在后续“看图说话”系列文章中出现）的朋友也不难看出，如此，我们更容易得到一个有意义的P值！
 点估计更准确。如果我们需要根据一个小样本数据来估计学生的平均身高。那么使用正态分布来拟合，很容易就受到离群异常值的影响而得到错误的估计。
 回归中应用t分布，可以得到更稳健的估计量（β值或OR值），这也是我们实现“稳健回归”的一个重要手段。
  通过下面一幅图，我们巩固一下t分布的“宽厚”： 与正态分布曲线（矮胖）比较，t分布以其高高的尾部（本图中不明显，参见上面自由度为1,2,3时所对应的图片），容忍了在横轴为9处的异常值，得到了更稳健的集中趋势估计值（均值1.11）和更紧凑的离散趋势估计值（标准差差0.15，又是正态分布的四分之一）。要知道，我们如果单单想通过增加样本量来将标准误（假设检验中使用的参数，标准差除以自由度的平方根）缩减到四分之一，需要16倍的样本量！可见，t分布当真是威力无穷！
PS：上述两幅图中的t分布曲线并不是频率学派应用t分布的常规套路（更像是贝叶斯学派的用法）。细心者可以发现，我们使用的t分布的自由度明显低于n - 1的自由度计算方法。这里的自由度是根据最大似然法估计出来的，用以更恰当地拟合数据的分布。虽然这与我们平时的用法不同，但小编觉得，这一点点不同不仅无伤大雅，反而更有利于大家深入理解t分布的特征——温良宽厚。
卡方分布的应用 本文来自http://www.cnblogs.com/baiboy/p/tjx11.html
  提到统计学，很多人认为是经济学或者数学的专利，与计算机并没有交集。诚然在传统学科中，其在以上学科发挥作用很大。然而随着科学技术的发展和机器智能的普及，统计学在机器智能中的作用越来越重要。本系列统计学学习基于李航的《统计学习方法》一书和一些基本的概率知识。</description>
    </item>
    
    <item>
      <title>L0,L1,L2以及核范数规则化</title>
      <link>https://aileenxie.github.io/2018/02l_norm/</link>
      <pubDate>Wed, 10 Oct 2018 19:17:39 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/02l_norm/</guid>
      <description>看到一篇讲解很详细的文章，边看边做笔记整理要点。
前言 监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。
最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。
 过拟合：参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力。 泛化能力强：测试误差也很小 范数规则化有两个作用：
1）保证模型尽可能的简单，避免过拟合。
2）约束模型特性，加入一些先验知识，例如稀疏、低秩等。
  目标函数 一般来说，监督学习可以看做最小化下面的目标函数：  第一项L(yi,f(xi;w)): 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。
 如果是Square loss,那就是最小二乘了； 如果是Hinge Loss，那就是著名的SVM了； 如果是exp-Loss，那就是牛逼的 Boosting了； 如果是log-Loss，那就是Logistic Regression了； 不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。
  第二项λΩ(w): 也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。
 本文讨论的即是“规则项Ω(w)”； 一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数； 论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等；    L0范数与L1范数 L0范数 L0范数（||W||0）是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。
L1范数 L1范数（||W||1）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。L1范数会使权值稀疏，它是L0范数的最优凸近似。
 Tips: 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微。  为什么不用L0，而用L1？  是因为L0范数很难优化求解（NP难问题）； 是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。 总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。  为什么要稀疏？  特征选择(Feature Selection)：去掉没有信息的特征，及将对应权重置0，防止无用特征对测试新样本的干扰； 可解释性(Interpretability)：如最初有1000个特征，回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b，通过学习，如果最后学习到只有5个非零的wi，那么就可以说影响患病率的主要就是这5个特征，医生就好分析多了。  L2范数 L2范数（||W||2）是指向量各元素的平方和然后求平方根。它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”（weight decay）它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。与L1范数不同，它不会让它等于0，而是接近于0。</description>
    </item>
    
    <item>
      <title>Hugo快速搭建博客</title>
      <link>https://aileenxie.github.io/2018/01hugoblog/</link>
      <pubDate>Fri, 05 Oct 2018 21:52:20 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/01hugoblog/</guid>
      <description>简单记录一下mac下用Hugo搭建博客的过程，以便日后查阅。感谢renyijiu的安利和帮助。
Hugo  一个用Go语言写的静态网站生成器 能把markdown转变成静态网页 内置web服务期，便于本地草稿调试  安装 Hugo  直接用Homebrew安装
brew install hugo  完成之后查看版本，我目前装的是0.49
hugo version  创建一个新站点 到你的目录下找个喜欢的地方执行语句
hugo new site myblog   会在当前目录生成一个叫“myblog”的目录，里面包含若干文件夹和一个config.toml文件:
▸ archetypes/
▸ content/ -&amp;gt;你写的markdown文章
▸ layouts/ -&amp;gt;网站的模板文件
▸ static/ -&amp;gt;放的是一些图片、css、js等资源
▸ data/
▸ themes/ -&amp;gt;放的是你之后添加的主题
config.toml -&amp;gt;网站的配置文件
添加主题  进入目录页
cd myblog  初始化为git项目，方便之后放入github或者任何仓库进行版本管理
git init  添加主题, 以LeaveIt为例
git submodule add https://github.com/liuzc/LeaveIt.git themes/LeaveIt  之后会在themes/目录下看到你添加的主题。
 修改配置文件(直接打开config.toml文件修改也是一样的)
echo &#39;theme = &amp;quot;LeaveIt&amp;quot;&#39; &amp;gt;&amp;gt; config.</description>
    </item>
    
  </channel>
</rss>