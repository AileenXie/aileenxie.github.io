<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aileen&#39;s Blog on Aileen&#39;s Blog</title>
    <link>https://aileenxie.github.io/</link>
    <description>Recent content in Aileen&#39;s Blog on Aileen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch-cn</language>
    <lastBuildDate>Wed, 10 Oct 2018 19:17:39 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>L0,L1,L2以及核范数规则化</title>
      <link>https://aileenxie.github.io/2018/03norm-introduction/</link>
      <pubDate>Wed, 10 Oct 2018 19:17:39 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/03norm-introduction/</guid>
      <description>

&lt;p&gt;看到一篇讲解很详细的&lt;a href=&#34;https://blog.csdn.net/zouxy09/article/details/24971995&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;文章&lt;/a&gt;，边看边做笔记整理要点。&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最小化误差&lt;/strong&gt;是为了让我们的模型拟合我们的训练数据，而&lt;strong&gt;规则化参数&lt;/strong&gt;是防止我们的模型过分拟合我们的训练数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;过拟合：参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力。&lt;/li&gt;
&lt;li&gt;泛化能力强：测试误差也很小&lt;/li&gt;

&lt;li&gt;&lt;p&gt;范数规则化有两个作用：&lt;/p&gt;

&lt;p&gt;1）保证模型尽可能的简单，避免过拟合。&lt;/p&gt;

&lt;p&gt;2）约束模型特性，加入一些先验知识，例如稀疏、低秩等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;目标函数&#34;&gt;目标函数&lt;/h2&gt;

&lt;p&gt;一般来说，监督学习可以看做最小化下面的目标函数：
&lt;img src=&#34;https://ws3.sinaimg.cn/large/006tNbRwly1fw2ykov1yxj30zw04adg7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;第一项L(yi,f(xi;w))&lt;/strong&gt;:
衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果是Square loss,那就是最小二乘了；&lt;/li&gt;
&lt;li&gt;如果是Hinge Loss，那就是著名的SVM了；&lt;/li&gt;
&lt;li&gt;如果是exp-Loss，那就是牛逼的 Boosting了；&lt;/li&gt;
&lt;li&gt;如果是log-Loss，那就是Logistic Regression了；&lt;/li&gt;
&lt;li&gt;不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;第二项λΩ(w)&lt;/strong&gt;:
也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本文讨论的即是“规则项Ω(w)”；&lt;/li&gt;
&lt;li&gt;一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数；&lt;/li&gt;
&lt;li&gt;论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等；
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;l0范数与l1范数&#34;&gt;L0范数与L1范数&lt;/h2&gt;

&lt;h3 id=&#34;l0范数&#34;&gt;L0范数&lt;/h3&gt;

&lt;p&gt;L0范数（||W||0）是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。&lt;/p&gt;

&lt;h3 id=&#34;l1范数&#34;&gt;L1范数&lt;/h3&gt;

&lt;p&gt;L1范数（||W||1）是指向量中各个元素绝对值之和，也有个美称叫&lt;strong&gt;“稀疏规则算子”（Lasso regularization）&lt;/strong&gt;。L1范数会使权值稀疏，它是L0范数的最优凸近似。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tips: 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;为什么不用l0-而用l1&#34;&gt;为什么不用L0，而用L1？&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;是因为L0范数很难优化求解（NP难问题）；&lt;/li&gt;
&lt;li&gt;是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。&lt;/li&gt;
&lt;li&gt;总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;为什么要稀疏&#34;&gt;为什么要稀疏？&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;特征选择(Feature Selection)：去掉没有信息的特征，及将对应权重置0，防止无用特征对测试新样本的干扰；&lt;/li&gt;
&lt;li&gt;可解释性(Interpretability)：如最初有1000个特征，回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b，通过学习，如果最后学习到只有5个非零的wi，那么就可以说影响患病率的主要就是这5个特征，医生就好分析多了。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;l2范数&#34;&gt;L2范数&lt;/h2&gt;

&lt;p&gt;L2范数（||W||2）是指向量各元素的平方和然后求平方根。它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”（weight decay）它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。与L1范数不同，它不会让它等于0，而是&lt;strong&gt;接近于0&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&#34;l2范数的好处&#34;&gt;L2范数的好处：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。&lt;/li&gt;
&lt;li&gt;从优化计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;condition-number&#34;&gt;Condition number&lt;/h3&gt;

&lt;p&gt;优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ill-condition病态：假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。&lt;/li&gt;
&lt;li&gt;condition number：用来衡量ill-condition系统的可信度的，即输入发生微小变化时，输出会发生多大变化。值越小越好。
如果方阵A是非奇异的，那么A的condition number定义为：
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tNbRwly1fw32a1nvcsj310403u74c.jpg&#34; alt=&#34;&#34; /&gt;
如果方阵A是奇异的，那么A的condigion number正无穷大。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;l2为什么能处理condition-number不好的情况&#34;&gt;L2为什么能处理condition number不好的情况？&lt;/h3&gt;

&lt;p&gt;因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tNbRwly1fw32cce7llj312205et8y.jpg&#34; alt=&#34;&#34; /&gt;
如果&lt;strong&gt;当我们的样本X的数目比每个样本的维度还要小的时候，矩阵XTX将会不是满秩的，也就是XTX会变得不可逆&lt;/strong&gt;，所以w*就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。总而言之，我们过拟合了。
但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了(？？？)：
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tNbRwly1fw32uzyn84j313003wjrp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。&lt;/li&gt;
&lt;li&gt;考虑没有规则项的时候，也就是λ=0的情况，如果矩阵XTX的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善condition number。&lt;/li&gt;
&lt;li&gt;如果使用迭代优化的算法，condition number太大还会拖慢迭代收敛的速度。规则项从优化的角度来看，实际上是将目标函数变成λ-strongly convex（λ强凸）的了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;强凸&#34;&gt;强凸&lt;/h3&gt;

&lt;p&gt;就是convex 可以保证函数在任意一点都处于它的一阶泰勒函数之上，而strongly convex可以保证函数在任意一点都存在一个非常漂亮的二次下界quadratic lower bound。
&lt;img src=&#34;https://ws4.sinaimg.cn/large/006tNbRwly1fw33tpbzohj30y80ekta8.jpg&#34; alt=&#34;&#34; /&gt;
正如上面分析的那样，如果f(w)在全局最小点w*周围是非常平坦的情况的话，我们有可能会找到一个很远的点。但如果我们有“强凸”的话，就能对情况做一些控制，我们就可以得到一个更好的近似解。&lt;/p&gt;

&lt;h3 id=&#34;l1和l2差别&#34;&gt;L1和L2差别&lt;/h3&gt;

&lt;h4 id=&#34;1-下降速度&#34;&gt;1.&lt;strong&gt;下降速度&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;L1就是按绝对值函数的“坡”下降的&lt;/li&gt;

&lt;li&gt;&lt;p&gt;L2是按二次函数的“坡”下降。&lt;/p&gt;

&lt;p&gt;所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。
&lt;img src=&#34;https://ws2.sinaimg.cn/large/006tNbRwly1fw3cmhrbx1j31ga0potb0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-模型空间的限制-l1-regularization-能产生稀疏性-而l2-regularization-不行&#34;&gt;2.&lt;strong&gt;模型空间的限制(L1-regularization 能产生稀疏性，而L2-regularization 不行)：&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;L1和L2规则化的代价函数形式如下：
  &lt;img src=&#34;https://ws3.sinaimg.cn/large/006tNbRwly1fw3bgb75yrj311e08odgq.jpg&#34; alt=&#34;&#34; /&gt;
  在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解：
  &lt;img src=&#34;https://ws2.sinaimg.cn/large/006tNbRwly1fw3bhq203sj313i0lmq7r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;L1-ball 与L2-ball 的不同就在于：
L1. 在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性;
L2. 没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;总结：&lt;/strong&gt;L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;核范数&#34;&gt;核范数&lt;/h2&gt;

&lt;p&gt;核范数||W||*是指矩阵奇异值的和，英文称呼叫Nuclear Norm。用来约束Low-Rank（低秩）。&lt;/p&gt;

&lt;h3 id=&#34;约束rank-w-与核范数有何关系&#34;&gt;约束rank（w）与核范数有何关系？&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;因为rank()是&lt;strong&gt;非凸的&lt;/strong&gt;，在优化问题里面很难求解，那么就需要寻找它的凸近似来近似它了。rank(w)的凸近似就是核范数||W||*。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;低秩的应用&#34;&gt;低秩的应用&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;矩阵填充(Matrix Completion)&lt;/li&gt;
&lt;li&gt;鲁棒PCA（鲁棒：只是假设它的噪声是稀疏的）&lt;/li&gt;
&lt;li&gt;背景建模&lt;/li&gt;
&lt;li&gt;变换不变低秩纹理（TILT）
具体讲解见&lt;a href=&#34;https://blog.csdn.net/zouxy09/article/details/24972869&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;参考文章&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;规则化参数的选择&#34;&gt;规则化参数的选择&lt;/h2&gt;

&lt;p&gt;重新审视我们的目标函数：
&lt;img src=&#34;https://ws2.sinaimg.cn/large/006tNbRwly1fw2yk725x5j30o8036wet.jpg&#34; alt=&#34;&#34; /&gt;
里面除了loss和规则项两块外，还有一个参数λ，叫hyper-parameters（超参）。它主要是平衡loss和规则项这两项的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;λ越大，就表示规则项要比模型训练误差更重要，也就是相比于要模型拟合我们的数据，我们更希望我们的模型能满足我们约束的Ω(w)的特性。&lt;/li&gt;
&lt;li&gt;λ越小，就表示希望输出与期待值误差最小（回归曲线尽量过所有点，就会过拟合）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;调参经验&#34;&gt;调参经验&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;如Hinton大哥的那篇A Practical Guide to Training RestrictedBoltzmann Machines等等；&lt;/li&gt;
&lt;li&gt;通过分析我们的模型来选择：
在训练之前，我们大概计算下这时候的loss项的值是多少？Ω(w)的值是多少？然后针对他们的比例来确定我们的λ，这种启发式的方法会缩小我们的搜索空间。&lt;/li&gt;
&lt;li&gt;交叉验证Cross validation：
先把我们的训练数据库分成几份，然后取一部分做训练集，一部分做测试集，然后选择不同的λ用这个训练集来训练N个模型，然后用这个测试集来测试我们的模型，取N模型里面的测试误差最小对应的λ来作为我们最终的λ。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Hugo快速搭建博客</title>
      <link>https://aileenxie.github.io/2018/hugo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Fri, 05 Oct 2018 21:52:20 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/hugo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>

&lt;p&gt;简单记录一下mac下用&lt;a href=&#34;https://gohugo.io/&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt;搭建博客的过程，以便日后查阅。感谢&lt;a href=&#34;https://blog.renyijiu.com/&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;renyijiu&lt;/a&gt;的安利和帮助。&lt;/p&gt;

&lt;h2 id=&#34;hugo&#34;&gt;Hugo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;一个用Go语言写的静态网站生成器&lt;/li&gt;
&lt;li&gt;能把markdown转变成静态网页&lt;/li&gt;
&lt;li&gt;内置web服务期，便于本地草稿调试&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装-hugo&#34;&gt;安装 Hugo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;直接用Homebrew安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;完成之后查看版本，我目前装的是0.49&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建一个新站点&#34;&gt;创建一个新站点&lt;/h2&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;到你的目录下找个喜欢的地方执行语句&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo new site myblog
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;会在当前目录生成一个叫“myblog”的目录，里面包含若干文件夹和一个config.toml文件:&lt;/p&gt;

&lt;p&gt;▸ archetypes/&lt;/p&gt;

&lt;p&gt;▸ content/ -&amp;gt;你写的markdown文章&lt;/p&gt;

&lt;p&gt;▸ layouts/ -&amp;gt;网站的模板文件&lt;/p&gt;

&lt;p&gt;▸ static/ -&amp;gt;放的是一些图片、css、js等资源&lt;/p&gt;

&lt;p&gt;▸ data/&lt;/p&gt;

&lt;p&gt;▸ themes/ -&amp;gt;放的是你之后添加的主题&lt;/p&gt;

&lt;p&gt;config.toml -&amp;gt;网站的配置文件&lt;/p&gt;

&lt;h2 id=&#34;添加主题&#34;&gt;添加主题&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进入目录页&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd myblog
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;初始化为git项目，方便之后放入github或者任何仓库进行版本管理&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git init
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加主题, 以LeaveIt为例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule add  https://github.com/liuzc/LeaveIt.git themes/LeaveIt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后会在themes/目录下看到你添加的主题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改配置文件(直接打开config.toml文件修改也是一样的)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;theme = &amp;quot;LeaveIt&amp;quot;&#39; &amp;gt;&amp;gt; config.toml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hugo中有很多可供选择的&lt;a href=&#34;https://themes.gohugo.io/&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;主题&lt;/a&gt;，选一个你喜欢的，复制链接替换上面的主题&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;添加第一篇博客&#34;&gt;添加第一篇博客&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在myblog目录下运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo new posts/my-first-post.md
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;会在content/posts目录下生成一个my-first-post.md的文件，文件初始会有&amp;rdquo;title&amp;rdquo;(文章标题)、&amp;rdquo;date&amp;rdquo;（文章时间）、&amp;rdquo;draft&amp;rdquo;（是否是草稿）几个字段内容。之后你要写的内容以markdown写在后面即可，这里我们可以先不管。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;【tips】: 刚开始我在这步运行报错，查了资料发现是hugo版本问题，需要更新到0.48以上，如果装的版本旧了，手动更新一下：&lt;/b&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew update
brew upgrade hugo
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;启动web服务器本地调试-看看效果&#34;&gt;启动web服务器本地调试（看看效果）&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在myblog目录下运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo server
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;访问：&lt;a href=&#34;http://localhost:1313/&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;http://localhost:1313/&lt;/a&gt;
可以看到博客的效果，值得一提的是，前面生成的.md文档里&amp;rdquo;draft&amp;rdquo;设置为&amp;rdquo;true&amp;rdquo;的，无法被看到。想要看到作为草稿的文章，记得加&amp;rdquo;-D&amp;rdquo;参数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo server -D
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;调整你的主题&#34;&gt;调整你的主题&lt;/h2&gt;

&lt;p&gt;按照所用主题的例子修改你的config.toml文件.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;公共的部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;baseURL = &amp;quot;https://example.org/&amp;quot;
languageCode = &amp;quot;en-us&amp;quot;
title = &amp;quot;My New Hugo Site&amp;quot;
theme = &amp;quot;ananke&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;所用主题的定制部分参考主题介绍，如LeaveIt的config.toml中的头像配置  &lt;code&gt;avatar = &amp;quot;/images/me/avatar.jpeg&amp;quot;&lt;/code&gt;把你想要的头像按照/images/meavatar.jpeg格式放到&lt;strong&gt;static&lt;/strong&gt;目录下（注意不是项目根目录）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;利用hugo生成静态页面&#34;&gt;利用Hugo生成静态页面&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在myblog目录下运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hugo
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;会自动生成/刷新&lt;strong&gt;public&lt;/strong&gt;目录，之后每次修改过markdown文件或改过主题，都要重新运行&lt;code&gt;hugo&lt;/code&gt;命令以更新public目录下的静态文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;【tips】:本地调试的时候不需要运行&lt;code&gt;hugo&lt;/code&gt;,修改保存后会直接更新在页面上，但public下的静态文件是没有被更新的。&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;将public目录代码放到github上&#34;&gt;将public目录代码放到Github上&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;github上新建一个项目&amp;rdquo;blog&amp;rdquo;&lt;/li&gt;
&lt;li&gt;复制项目ssh链接地址: 如git@github.com:xxx/blog.git&lt;/li&gt;

&lt;li&gt;&lt;p&gt;确保已上传ssh-key，进入本地myblog项目的public目录初始化git并添加远端仓库&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd public
git init
git remote add origin git@github.com:xxx/blog.git
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提交你public下的代码&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git add .
git commit -m &#39;initial commit.&#39;
git push origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;设置blog项目的&amp;rdquo;Repository name&amp;rdquo;为&amp;rdquo;xxx.github.io&amp;rdquo;(xxx是你的github用户名），通过github的page功能，你的博客就可以通过&lt;a href=&#34;https://xxx.github.io/&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;https://xxx.github.io/&lt;/a&gt;直接访问。（只提交public目录即可直接访问不用再做跳转。）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个过程到此结束，之后的博客更新就是直接修改content/目录下的markdown文件，然后执行&lt;code&gt;hugo&lt;/code&gt;后push更新到github。大功告成！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://aileenxie.github.io/about/</link>
      <pubDate>Fri, 05 Oct 2018 13:01:36 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/about/</guid>
      <description>

&lt;h3 id=&#34;我&#34;&gt;我&lt;/h3&gt;

&lt;p&gt;自动化测试跨界机器学习的工作后女研究僧，90后，爱学习爱生活&lt;/p&gt;

&lt;h3 id=&#34;博客&#34;&gt;博客&lt;/h3&gt;

&lt;p&gt;记录一些学习笔记和心得感悟&lt;/p&gt;

&lt;h3 id=&#34;用过的&#34;&gt;用过的&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Java/Groovy+Spock&lt;/li&gt;
&lt;li&gt;Ruby+Cucumber+Capybara&lt;/li&gt;
&lt;li&gt;Selenium/Appium/Jmeter&lt;/li&gt;
&lt;li&gt;MySQL/PostgreSQL&lt;/li&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;在学的&#34;&gt;在学的&lt;/h3&gt;

&lt;p&gt;机器学习、深度学习、计算机视觉相关&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Pytorch/TensorFlow&lt;/li&gt;
&lt;li&gt;RNN/CNN/DNN&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>New Start</title>
      <link>https://aileenxie.github.io/2018/first-post/</link>
      <pubDate>Fri, 05 Oct 2018 13:01:24 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/first-post/</guid>
      <description>

&lt;p&gt;重新回到学校一个月，第一个国庆避开人山人海，在上海安安心心休息了几天。&lt;/p&gt;

&lt;p&gt;之前工作的时候一直想要搭一个博客，把每天零零碎碎记录的笔记整理出来放在一起，但一直没能着手开始行动（大概是又忙又懒。。）直到笔记随手记了一大堆，越来越不想整理。&lt;/p&gt;

&lt;p&gt;回到学校自由时间多了很多，又在学习新东西，想着这是个好的时机开始写博客，终于在这个国庆开始了这项活动。&lt;/p&gt;

&lt;h3 id=&#34;关于笔记&#34;&gt;关于笔记&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;最开始一年我用的ubuntu系统，被安利了&lt;strong&gt;cherrytree&lt;/strong&gt;，试用了一下，马上被这种可以“遍地开花”随手快速记录的树状结构笔记软件所吸引。
那年我刚工作，又是从通信跨到互联网行业，很多东西对我来说都是新知识：数据库(Mysql/Postgres)、编程语言（JAVA/Groov/Python)、测试框架（Spock/Selenium/Appium/rf/jmeter)、版本管理系统（Git）、持续集成（Gradle/Jenkins）包括Linux系统本身&amp;hellip; 用cherrytree记了相当多的笔记，这一年也是我收获颇丰的一年。&lt;/li&gt;
&lt;li&gt;第二年换了macOS系统，cherrytree用不了（用模拟器打开页面很丑），先后换了有道云笔记，为知笔记，最后定格在了bootsnote（页面简洁好看，支持markdown和snippet）。又陆续做了关于ruby系测试框架（Cucumber/Capybara）、mac系统等的笔记。直到开学，我作为测试开发的工作告一段落。笔记也就此暂停。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;关于博客&#34;&gt;关于博客&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;终于展开新的研究生生活，投身于机器学习、深度学习和计算机视觉方向研究。有种可以大展拳脚的感觉略兴奋。从开学前到现在，看过了CS229吴恩达的机器学习基础课程，CS231n斯坦福卷积神经网络课程，学了pytorch框架，开始看导师给的paper。开始看的很细致做的手写的笔记，现在想来还是要有个地方集中放置一下，就从现在开始重新规划一下之后的笔记。预计会放一些paper的阅读笔记，学习的知识点，或许是任何想记下来的东西吧hhh&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;七天假期马上要过去啦，搭好了博客算是一个很好的开始，希望能我好好充实这个博客，记录整个研究生阶段以及未来和以后更远更远的学习旅程，(๑•̀ㅂ•́)و✧加油！！&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>