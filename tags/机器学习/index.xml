<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Aileen&#39;s Blog</title>
    <link>https://aileenxie.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Aileen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch-cn</language>
    <lastBuildDate>Fri, 19 Oct 2018 15:07:13 +0800</lastBuildDate>
    
	<atom:link href="https://aileenxie.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>近端梯度（Proximal Gradient, PG）算法详解</title>
      <link>https://aileenxie.github.io/2018/03pg/</link>
      <pubDate>Fri, 19 Oct 2018 15:07:13 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/03pg/</guid>
      <description>--  --  --  PG算法 简介 Proximal算法是用于求解凸优化问题的方法之一。用于求解目标函数形如f(x)=g(x)+h(x)（其中g(x)可微而h(x)不可微）形式无约束问题的下降算法。
 当无约束的凸优化问题可微，我们可以用梯度下降算法求解； 当无约束的凸优化目标函数不可微，我们可以采用次梯度(subgradient)算法求解(该算法时间复杂度较高,且不会产生稀疏)； 当存在约束时，我们可以采用proximal相关梯度算法求解(可降低复杂度至O(1/ϵ));  这是因为，当目标函数存在约束时，我们可以把约束写入目标函数，但是这时候往往目标函数就从可微变成不可微，例如线性回归加入L−1norm，记为\( ∥y−xβ∥_2^2+λ∥β∥_1\) ，那么很明显，目标函数前半部分为凸且连续可微，但后半部分为凸但不连续可微。
算法模型 其中g(x)为凸函数，可微。h(x)也是凸函数，但不可微。其中g(x),h(x)是由F(x) 分离出来的两项，当F(x) 分离的结果不同，即使是同一个问题，算法的实现方式也不尽相同，
基本概念 【临近算子】（proximity operator） &amp;gt; 当h(x)=0,则prox\(h\)(x)=\(arg \min _{u}\)(\(\frac{1}{2}||u−x||_2^2\))=x; &amp;gt; 当h(x)=Ic，则prox\(h\)(x)=\(arg \min _{u∈c}\)(\(\frac{1}{2}||u−x||_2^2\))=Pc(x); &amp;gt; 当h(x)=\(t||X||_1\)，则prox\(h(x)\) 为软阈值算法。 临近算子是对梯度的延伸，只与h(x)有关，当函数h(x)为光滑函数时，该临近算子就是梯度。
F(x)的【近端梯度】（proximal gradient） 近端梯度就是对F(x)梯度的近似，其中： 集合X的【指示函数】（indicator function） 其中X是一个凸集合。利用指示函数，我们可以将有约束问题写成无约束问题,如下：
当x不在X中，等式为无穷大，因此x肯定不是最优值。因此就等于限定了x在凸集合X中。
变量x在集合X上的【投影算子】（projection operator） 投影的含义：一个点x在集合X上的投影，就是X上离x的欧几里得距离最近的点。
Tips:
 2个在集合外的点x，y之间的距离，一定大于这两点在凸集合X上的投影的距离 当h(x)=\(l_{X}(x)\)时，\(proj_x(x)=prox_h(x)\)。  算法实现 迭代计算最优解x，直到F(x)在最小值附近收敛。
算法伪代码： 资料：
Proximal Algorithms
Projected Gradient Method and LASSO 
CSDN-Proximal Gradient Method近端梯度算法
近端梯度法(Proximal Gradient Method, PG)</description>
    </item>
    
    <item>
      <title>L0,L1,L2以及核范数规则化</title>
      <link>https://aileenxie.github.io/2018/02l_norm/</link>
      <pubDate>Wed, 10 Oct 2018 19:17:39 +0800</pubDate>
      
      <guid>https://aileenxie.github.io/2018/02l_norm/</guid>
      <description>看到一篇讲解很详细的文章，边看边做笔记整理要点。
前言 监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。
最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。
 过拟合：参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力。 泛化能力强：测试误差也很小 范数规则化有两个作用：
1）保证模型尽可能的简单，避免过拟合。
2）约束模型特性，加入一些先验知识，例如稀疏、低秩等。
  目标函数 一般来说，监督学习可以看做最小化下面的目标函数：  第一项L(yi,f(xi;w)): 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。
 如果是Square loss,那就是最小二乘了； 如果是Hinge Loss，那就是著名的SVM了； 如果是exp-Loss，那就是牛逼的 Boosting了； 如果是log-Loss，那就是Logistic Regression了； 不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。
  第二项λΩ(w): 也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。
 本文讨论的即是“规则项Ω(w)”； 一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数； 论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等；    L0范数与L1范数 L0范数 L0范数（||W||0）是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。
L1范数 L1范数（||W||1）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。L1范数会使权值稀疏，它是L0范数的最优凸近似。
 Tips: 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微。  为什么不用L0，而用L1？  是因为L0范数很难优化求解（NP难问题）； 是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。 总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。  为什么要稀疏？  特征选择(Feature Selection)：去掉没有信息的特征，及将对应权重置0，防止无用特征对测试新样本的干扰； 可解释性(Interpretability)：如最初有1000个特征，回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b，通过学习，如果最后学习到只有5个非零的wi，那么就可以说影响患病率的主要就是这5个特征，医生就好分析多了。  L2范数 L2范数（||W||2）是指向量各元素的平方和然后求平方根。它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”（weight decay）它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。与L1范数不同，它不会让它等于0，而是接近于0。</description>
    </item>
    
  </channel>
</rss>